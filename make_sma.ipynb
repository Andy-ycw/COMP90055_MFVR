{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "from sma_tools import flatten_mf_dict, MFDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import jsonpickle\n",
    "from emfdscore.load_mfds import * \n",
    "import numpy as np\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sentences in voice_dict through BERT base uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "key_name = 'mfd_bert_uncased'\n",
    "file_path = './data/pickle_240227/voice_mbu.pickle'\n",
    "with open(file_path, 'r') as f:\n",
    "    voice_dict = jsonpickle.decode(f.read())\n",
    "# voice_sent_flatten = flatten_mf_dict(voice_dict, key_name, _size=32)\n",
    "key_list, voice_sent_flatten = flatten_mf_dict(voice_dict, key_name)\n",
    "\n",
    "# Model specs\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name).to(mps_device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47548"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voice_sent_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the contextual representation of mfw\n",
    "voice_dataset = MFDataset(voice_sent_flatten)\n",
    "data_loader = DataLoader(voice_dataset, batch_size=batch_size)\n",
    "\n",
    "sents_contrepr = []\n",
    "for batch in data_loader: # ~7.5 mins for the voice dict\n",
    "    sents_batch = batch\n",
    "    tokenized_batch = tokenizer(\n",
    "        sents_batch, \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        add_special_tokens=False, # CLS token is not needed in sma construction.\n",
    "        return_tensors=\"pt\")\n",
    "    input_ids = tokenized_batch['input_ids']    \n",
    "    attention_mask = tokenized_batch['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_output = model(input_ids.to(mps_device), attention_mask=attention_mask.to(mps_device))\n",
    "        batch_context_output = batch_output.last_hidden_state.cpu().numpy()\n",
    "        for sent_contrepr in batch_context_output:\n",
    "            assert sent_contrepr.ndim == 2, f\"The dimension is unexpecuted: {sent_contrepr.ndim}\"\n",
    "            sents_contrepr.append(sent_contrepr) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4541\n",
      "Length of the input sentence list: 47548\n",
      "Sanity check for the length of output coolected: 47548\n",
      "Sanity check context vectors for a sentence: (768,)\n"
     ]
    }
   ],
   "source": [
    "print(len(key_list))\n",
    "print('Length of the input sentence list:', len(voice_dataset))\n",
    "print('Sanity check for the length of output coolected:', len(sents_contrepr))\n",
    "print('Sanity check context vectors for a sentence/token:', sents_contrepr[0][83].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate moral foundation token vectors under each foundation for MDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vice virtue code in mdf\n",
    "mdf_vv_code = set()\n",
    "for v in mfd.values():\n",
    "    for v2 in v:\n",
    "        mdf_vv_code.add(v2)\n",
    "\n",
    "sma_aggregation = {k: [] for k in mdf_vv_code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_counter = 0\n",
    "for key_num in range(len(key_list)):\n",
    "    key = key_list[key_num]\n",
    "    mf_info = voice_dict[key][key_name]\n",
    "\n",
    "    # A collection below indicates that the data included belong to multiple sentences.\n",
    "    token_collection = mf_info[2] \n",
    "    vice_virtue_collection = mf_info[3]\n",
    "    assert len(token_collection) == len(vice_virtue_collection), f\"The lengths of token_collection {len(token_collection)} and vice_virtue_collection {len(vice_virtue_collection)} should be the same.\"\n",
    "    \n",
    "    # loop through each sentence with mf words\n",
    "    for i in range(len(token_collection)):\n",
    "        sent_vector = sents_contrepr[sentence_list_counter]\n",
    "        idx_list = token_collection[i] \n",
    "        vv_list = vice_virtue_collection[i] # This is a list\n",
    "\n",
    "        # The below code runs for every token identified as MF words previously.\n",
    "        for j in range(len(idx_list)):\n",
    "            idx = idx_list[j] # This is int\n",
    "            vv = vv_list[j] # This is a list. As a token can be assigned multiple foundations.\n",
    "\n",
    "            try:\n",
    "                target_vector = sent_vector[idx]\n",
    "            except IndexError: # Caused by abnormally long sentences which were probably truncated when running the sentence through BERT\n",
    "                continue\n",
    "            for v in vv: # Sorry for the confusion caused by the use of `v` here.\n",
    "                sma_aggregation[v].append(target_vector)\n",
    "    \n",
    "        sentence_list_counter += 1\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairness.virtue 8937\n",
      "authority.virtue 26582\n",
      "sanctity.vice 996\n",
      "fairness.vice 1694\n",
      "loyalty.vice 1879\n",
      "authority.vice 1756\n",
      "loyalty.virtue 43781\n",
      "care.vice 6776\n",
      "moral 12629\n",
      "sanctity.virtue 2123\n",
      "care.virtue 5707\n"
     ]
    }
   ],
   "source": [
    "for k in sma_aggregation:\n",
    "    print(k, len(sma_aggregation[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47548"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4540"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_aggregation['fairness.vice'][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90055",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
